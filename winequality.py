# -*- coding: utf-8 -*-
"""ASProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DAnpdwiR80G2bqbXNPZOnwSwdbo_KRtj
"""

#Import all needed libraries

import pandas as pd
import numpy as np
import time
import math
import pprint as pprint

from sklearn import tree
import matplotlib.pyplot as plt 
import seaborn as sns
from sklearn import metrics
from sklearn.model_selection import train_test_split 
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import KNeighborsRegressor
from sklearn import svm
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix 
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.model_selection import KFold

#Read csv files

red_wine = pd.read_csv('winequality-red.csv',sep=';')
white_wine = pd.read_csv('winequality-white.csv',sep=';')

l_red = len(red_wine)
l_white = len(white_wine)

define_red = np.repeat('red', l_red)
define_white = np.repeat('white', l_white)

#Add the wine_type colomn
red_wine['wine_type'] = define_red
white_wine['wine_type'] = define_white

#Appending dataset into the other and checking for null values
data = red_wine.append(white_wine)
data = data.dropna()

print("Null values: " + str(data.isnull().values.any()))
print(data.info())

#Plotting the data into graph and checking the amount for each class
plt.bar(['White wine','Red wine'],list(data['wine_type'].value_counts()))
plt.ylabel("No. of wines")
plt.show()

white_w= data[data['wine_type']=='white']
red_w = data[data['wine_type']=='red']
total = sum(data['wine_type'].value_counts())

print("White wines: ", round((white_w.shape[0]/total)*100,3), end='%\n')
print("Red wine: ", red_w.shape[0])
print("White wine: ", white_w.shape[0])

#Visualize the first elements of the dataset
data.head(20)

#Visualize correlations between the features of the dataset
fig=plt.figure(figsize= (8, 6)) 
sns.heatmap(data.corr(), cmap='Reds')
plt.show()

#Splitting the data into training and testing (75/25)
array = data.values
X = array[:,0:12]
Y = array[:,12]
X_train, X_test, Y_train, Y_test=train_test_split(X, Y, test_size=0.25, random_state=42)
print(X_train.shape, Y_train.shape)

print(X_test.shape, Y_test.shape)

"""K-Nearest Neighbours Classifier, Decision Tree Classifier, Random Forest Classifier and Support Vector Machine

# We define all the models

## Classifiers
"""

#CLASSIFIERS
t = dict()
# KNeighbors training and prediction (timed)
start = time.time()
kneigh = KNeighborsClassifier()
kneigh.fit(X_train, Y_train)
kneighPred = kneigh.predict(X_test)

end = time.time()
final = end-start
t['KNNeigh'] = final
print('\n Elapsed time (s): {}.'.format(final))

# KNeighbors Tunned training and prediction (timed)
start = time.time()
# CV grid
cv_grid = KFold(n_splits=2, shuffle=True, random_state=0)
# Search space
param_grid = {'n_neighbors':list(range(1,11)),
             'weights':["distance", "uniform"],
             'p': [1,2]
             }
budget = 20
np.random.seed(0)
randomNeigh = RandomizedSearchCV(KNeighborsClassifier(), 
                         param_grid,
                         cv=cv_grid, 
                         n_jobs=1, verbose=1,
                         n_iter=budget
                        )
randomNeigh.fit(X=X_train, y=Y_train)
randomNeigh_pred = randomNeigh.predict(X_test)
# End the timer
end = time.time()
final = end - start
t['KNeighTunned'] = final
print("Best parameters :", randomNeigh.best_params_)

#DecisionTree training and prediction (timed)
start = time.time()
decisionTree = DecisionTreeClassifier()
decisionTree.fit(X_train, Y_train)
decisionTreePred = decisionTree.predict(X_test)
end = time.time()

final = end-start
t['DecisionTree'] = final
print('\n Elapsed time (s): {}.'.format(final))

#Random Forest training and prediction (timed)
start = time.time()
randomForest = RandomForestClassifier()
randomForest.fit(X_train, Y_train) 
randomForestPred=randomForest.predict(X_test)
end = time.time()

final = end-start
t['RandomForest'] = final
print('\n Elapsed time (s): {}.'.format(final))

#Random Forest Tunned training and prediction (timed)
start = time.time()

param_grid = [
{'n_estimators': [3, 10, 30]},
{'bootstrap': [False], 'n_estimators': [3, 10]},
]
randomForestTunned = RandomForestClassifier()
gridRandom = GridSearchCV(randomForestTunned, param_grid, cv=5)
gridRandom.fit(X_train, Y_train)
randomTunnedPred = gridRandom.predict(X_test)

# End the timer
end = time.time()
final = end - start
t['RandomTunned'] = final
print("Best parameters :", gridRandom.best_params_)

#SVC training and prediction (timed)
start = time.time()
svc = svm.SVC()
svc.fit(X_train, Y_train)
svcPred = svc.predict(X_test)
end = time.time()

final = end-start
t['SVC'] = final
print('\n Elapsed time (s): {}.'.format(final))

"""### Measurement

"""

# PRECISION SCORE
precision = dict()
precision['KNNeigh'] = precision_score(Y_test,kneighPred, average="binary", pos_label='red')
precision['KNNeighTunned'] = precision_score(Y_test,randomNeigh_pred, average="binary", pos_label='red')
precision['DecisionTree'] = precision_score(Y_test, decisionTreePred, average="binary", pos_label='red')
precision['RandomForest'] = precision_score(Y_test,randomForestPred, average="binary", pos_label='red')
precision['RandomForestTunned'] = precision_score(Y_test,randomTunnedPred, average="binary", pos_label='red')
precision['SVC'] = precision_score(Y_test, svcPred, average="binary", pos_label='red')

recall = dict()
recall['KNNeigh'] = recall_score(Y_test,kneighPred, average="binary", pos_label='red')
recall['KNNeighTunned'] = recall_score(Y_test,randomNeigh_pred, average="binary", pos_label='red')
recall['DecisionTree'] = recall_score(Y_test, decisionTreePred, average="binary", pos_label='red')
recall['RandomForest'] = recall_score(Y_test,randomForestPred, average="binary", pos_label='red')
recall['RandomForestTunned'] = recall_score(Y_test,randomTunnedPred, average="binary", pos_label='red')
recall['SVC'] = recall_score(Y_test, svcPred, average="binary", pos_label='red')

f1 = dict()
f1['KNNeigh'] = f1_score(Y_test,kneighPred, average="binary", pos_label='red')
f1['KNNeighTunned'] = f1_score(Y_test,randomNeigh_pred, average="binary", pos_label='red')
f1['DecisionTree'] = f1_score(Y_test, decisionTreePred, average="binary", pos_label='red')
f1['RandomForest'] = f1_score(Y_test,randomForestPred, average="binary", pos_label='red')
f1['RandomForestTunned'] = f1_score(Y_test,randomTunnedPred, average="binary", pos_label='red')
f1['SVC'] = f1_score(Y_test, svcPred, average="binary", pos_label='red')

#We print all the scores for Precision, Recall and F1
pp = pprint.PrettyPrinter(indent=4)
print("Precision\n")
pp.pprint(precision)

print("Recall\n")
pp.pprint(recall)

print("F1\n")
pp.pprint(f1)

print("Time\n")
pp.pprint(t)

#We create another dictionary for the confusion matrix
matrix = dict()
matrix['KNNeigh'] = confusion_matrix(Y_test, kneighPred)
matrix['KNNeighTunned'] = confusion_matrix(Y_test, randomNeigh_pred)
matrix['DecisionTree'] = confusion_matrix(Y_test, decisionTreePred)
matrix['RandomForest'] = confusion_matrix(Y_test, randomForestPred)
matrix['RandomForestTunned'] = confusion_matrix(Y_test, randomTunnedPred)
matrix['SVC'] = confusion_matrix(Y_test, svcPred)

pp.pprint(matrix)

#Confusion Matrix K-Neighbors
labels= ['Red wine', 'White wine'] 
matri=confusion_matrix(Y_test, kneighPred) 
plt.figure(figsize=(4, 4)) 
sns.heatmap((matri), xticklabels= labels, yticklabels= labels, annot=True, fmt="d")
plt.title("K- Neighbors Classifier - Confusion Matrix") 
plt.ylabel('Real Value') 
plt.xlabel('Prediction') 
plt.show()

#Confusion Matrix K-Neighbors Tunned
labels= ['Red wine', 'White wine'] 
matrix=confusion_matrix(Y_test, randomNeigh_pred) 
plt.figure(figsize=(4, 4)) 
sns.heatmap(matrix, xticklabels= labels, yticklabels= labels, annot=True, fmt="d")
plt.title("KN Neighbors Tunned Classifier - Confusion Matrix") 
plt.ylabel('Real Value') 
plt.xlabel('Prediction') 
plt.show()

#Confusion Matrix Decision Tree
labels= ['Red wine', 'White wine'] 
matrix=confusion_matrix(Y_test, decisionTreePred) 
plt.figure(figsize=(4, 4)) 
sns.heatmap(matrix, xticklabels= labels, yticklabels= labels, annot=True, fmt="d")
plt.title("Decision Tree Classifier - Confusion Matrix") 
plt.ylabel('Real Value') 
plt.xlabel('Prediction') 
plt.show()

#Confusion Matrix Random Forest
labels= ['Red wine', 'White wine'] 
matrix=confusion_matrix(Y_test, randomForestPred) 
plt.figure(figsize=(4, 4)) 
sns.heatmap(matrix, xticklabels= labels, yticklabels= labels, annot=True, fmt="d")
plt.title("Random Forest Classifier - Confusion Matrix") 
plt.ylabel('Real Value') 
plt.xlabel('Prediction') 
plt.show()

#Confusion Matrix Random Forest Tunned
labels= ['Red wine', 'White wine'] 
matrix=confusion_matrix(Y_test, randomTunnedPred) 
plt.figure(figsize=(4, 4)) 
sns.heatmap(matrix, xticklabels= labels, yticklabels= labels, annot=True, fmt="d")
plt.title("Random Forest Classifier - Confusion Matrix") 
plt.ylabel('Real Value') 
plt.xlabel('Prediction') 
plt.show()

#Confusion Matrix SVC
labels= ['Red wine', 'White wine'] 
matrix=confusion_matrix(Y_test, svcPred) 
plt.figure(figsize=(4, 4)) 
sns.heatmap(matrix, xticklabels= labels, yticklabels= labels, annot=True, fmt="d")
plt.title("SVC Classifier - Confusion Matrix") 
plt.ylabel('Real Value') 
plt.xlabel('Prediction') 
plt.show()

"""## Regressors"""

#Setting dataset for regression problem
array = data.values
X = array[:,0:12]
Y = array[:,11]
X_train, X_test, Y_train, Y_test=train_test_split(X, Y, test_size=0.2, random_state=42)
print(X_train.shape, Y_train.shape)
print(X_test.shape, Y_test.shape)

#REGRESSORS
tr = dict()
rmse = dict()
# KNeighbors
start = time.time()
kneigh = KNeighborsRegressor()
kneigh.fit(X_train, Y_train)
kneighPred = kneigh.predict(X_test)

end = time.time()

final = end-start
tr['KNNeigh'] = final
rmse['KNNeigh'] = math.sqrt(abs(metrics.mean_squared_error(Y_test, kneighPred)))

#DecisionTree
start = time.time()
decisionTree = DecisionTreeRegressor()
decisionTree.fit(X_train, Y_train)
decisionTreePred = decisionTree.predict(X_test)
end = time.time()

final = end-start
tr['DecisionTree'] = final
rmse['DecisionTree'] = math.sqrt(abs(metrics.mean_squared_error(Y_test, decisionTreePred)))

#Random Forest
start = time.time()
randomForest = RandomForestRegressor()
randomForest.fit(X_train, Y_train) 
randomForestPred=randomForest.predict(X_test)
end = time.time()

final = end-start
tr['RandomForest'] = final
rmse['RandomForest'] = math.sqrt(abs(metrics.mean_squared_error(Y_test, randomForestPred)))

#SVR
start = time.time()
svr = svm.SVR()
svr.fit(X_train, Y_train)
svrPred = svr.predict(X_test)
end = time.time()

final = end-start
tr['SVR'] = final
rmse['SVR'] = math.sqrt(abs(metrics.mean_squared_error(Y_test, svrPred)))

print("RMSE")
pp.pprint(rmse)
print("Time")
pp.pprint(tr)